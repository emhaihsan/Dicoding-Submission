# -*- coding: utf-8 -*-
"""Final Project: Image Classification Model Deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TlAOBAVxQ5b0cWg8057P3yUmrdilclfq

## Image Classification with Cifar-10 Dataset
![](https://production-media.paperswithcode.com/datasets/CIFAR-100-0000000433-b71f61c0_hPEzMRg.jpg)
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import models
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout, BatchNormalization

# Load dataset CIFAR-10
(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

train_images.shape, train_labels.shape, test_images.shape, test_labels.shape

"""Terdapat 50000 data latih dan 10000 data tes dengan ukuran gambar 32 x 32"""

# Normalisasi data gambar
# One Hot Encoding pada data labels
# Konversi ke tf tensor
num_classes = 10 # Data cifar-100 memiliki 100 label
train_images = train_images / 255.0
test_images = test_images / 255.0
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

train_images.shape, train_labels.shape, test_images.shape, test_labels.shape

"""Rekonstruksi data latih dan data tes agar memenuhi syarat data tes menggunakan 20% dari keseluruhan data"""

# Gabungkan data latih dan data tes
all_data = np.concatenate((train_images, test_images), axis=0)
all_labels = np.concatenate((train_labels, test_labels), axis=0)
# Pisahkan kembali menggunakan train test split
train_images, test_images, train_labels, test_labels = train_test_split(all_data, all_labels, test_size=0.2, random_state=42)

# Cek jumlah data
train_images.shape, train_labels.shape, test_images.shape, test_labels.shape

"""Data sudah dibagi dengan benar, 80% data latih, 20% data tes"""

# Konversi ke bentuk tf data
train_images = tf.constant(train_images)
train_labels = tf.constant(train_labels)
test_images = tf.constant(test_images)
test_labels = tf.constant(test_labels)

# Definisikan model
# Menggunakan model sekuensial
model = models.Sequential()
model.add(Conv2D(32, 3, activation='relu', padding='same',input_shape=(32,32,3)))
model.add(BatchNormalization())
model.add(Conv2D(32, 3, activation='relu', padding='same',))
model.add(BatchNormalization())
model.add(MaxPooling2D())
model.add(Dropout(0.5))

model.add(Conv2D(64, 3, activation='relu', padding='same',))
model.add(BatchNormalization())
model.add(Conv2D(64, 3, activation='relu', padding='same',))
model.add(BatchNormalization())
model.add(MaxPooling2D())
model.add(Dropout(0.5))

model.add(Conv2D(128, 3, activation='relu', padding='same',))
model.add(BatchNormalization())
model.add(Conv2D(128, 3, activation='relu', padding='same',))
model.add(BatchNormalization())
model.add(MaxPooling2D())
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

model.summary()

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Membuat Callback
# ModelCheckpoint untuk menyimpan model terbaik
# EarlyStopping untuk menghentikan training ketika performa model sudah tidak meningkat
# accuracy_callback untuk menghentikan training ketika performa model sudah mencapai target (val_accucary > 92%)
class valTarget(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if logs.get('val_accuracy') > 0.92:
            print(f"\nReached {0.92*100}% validation accuracy. Stopping training.")
            self.model.stop_training = True

checkpoint = ModelCheckpoint("model.h5", monitor='val_accuracy', save_best_only=True)
early_stopping = EarlyStopping(monitor='val_loss', patience=5)
accuracy_callback = valTarget()

# Train the model
history = model.fit(train_images, train_labels, epochs=100,
                    validation_split=0.2,
                    callbacks=[checkpoint, early_stopping, accuracy_callback])

# Plot history pelatihan
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.show()

# Tes load model yang disimpan dan gunakan untuk melakukan evaluasi pada data tes
model = tf.keras.models.load_model('model.h5')
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f'Test accuracy: {test_acc}')

# Simpan model dalam format tflite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)